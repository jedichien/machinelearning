{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer Introduction\n",
    "Optimizer在machine-learning中常被用來優化 loss function (ai的目標就是找到最小的loss，也就是最低的錯誤率，有關 loss function 將會在 loss_function 部分詳細說明)。<br/><br/>\n",
    "先來給大家帶個情境，某天你去爬山，不幸的所有設備都失靈了，你要如何下山呢? 對，你會往看起來像是山腳處地方走，也就是讓你抵達山腳失敗率(loss function)達到最低，這就是optimizer的作用(Gradient Descent)。<br/>\n",
    "<p align='center'>\n",
    "<img src='data/mountain.jpg' width='30%' height='20%' />\n",
    "</p><br>\n",
    "在這個notebook中，我將會向大家介紹 4種 下山的方式:\n",
    "#### SGD (Stochastic Gradient Descent)\n",
    "#### Momentum SGD\n",
    "#### RMSProp\n",
    "#### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD (Stochastic Gradient Descent)\n",
    "這個方式概念最簡單，基本上就是哪邊較低往哪邊走。<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式:<br/>\n",
    "$$\\theta = \\theta - \\alpha * \\frac{\\mathrm{d}J(\\theta)}{\\mathrm{d}\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta: 目前所在地$$\n",
    "$$\\alpha: 下山步伐$$\n",
    "$$\\frac{\\mathrm{d}J(\\theta)}{\\mathrm{d}\\theta}: 目前所在地的梯度$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們目標就是使$$\\theta 不再變化、梯度為0，也就是抵達地面。$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
